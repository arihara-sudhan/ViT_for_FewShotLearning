{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test on Data with Intraclass Variance : ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-09T08:53:45.628286Z",
     "iopub.status.busy": "2023-11-09T08:53:45.627535Z",
     "iopub.status.idle": "2023-11-09T08:53:45.635778Z",
     "shell.execute_reply": "2023-11-09T08:53:45.634727Z",
     "shell.execute_reply.started": "2023-11-09T08:53:45.628251Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "np.object = np.object_\n",
    "np.int = np.int_\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIPLETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:53:59.951451Z",
     "iopub.status.busy": "2023-11-09T08:53:59.950759Z",
     "iopub.status.idle": "2023-11-09T08:53:59.966514Z",
     "shell.execute_reply": "2023-11-09T08:53:59.965433Z",
     "shell.execute_reply.started": "2023-11-09T08:53:59.951418Z"
    }
   },
   "outputs": [],
   "source": [
    "class Triplet:\n",
    "    def __init__(self, train_folder):\n",
    "        self.train_folder = train_folder\n",
    "        self.labels = [label for label in os.listdir(train_folder) if label != '.ipynb_checkpoints']\n",
    "        self.label_to_path = {label: os.path.join(train_folder, label) for label in self.labels}\n",
    "\n",
    "    def get_triplet(self):\n",
    "        anchor_label = random.choice(self.labels)\n",
    "        anchor_path = random.choice(os.listdir(self.label_to_path[anchor_label]))\n",
    "        positive_label = anchor_label\n",
    "        positive_path = random.choice(os.listdir(self.label_to_path[positive_label]))\n",
    "        negative_label = random.choice([label for label in self.labels if label != anchor_label])\n",
    "        negative_path = random.choice(os.listdir(self.label_to_path[negative_label]))\n",
    "\n",
    "        anchor_image = os.path.join(self.label_to_path[anchor_label], anchor_path)\n",
    "        positive_image = os.path.join(self.label_to_path[positive_label], positive_path)\n",
    "        negative_image = os.path.join(self.label_to_path[negative_label], negative_path)\n",
    "\n",
    "        anchor_label_num = self.labels.index(anchor_label)\n",
    "        positive_label_num = self.labels.index(positive_label)\n",
    "        negative_label_num = self.labels.index(negative_label)\n",
    "\n",
    "        return anchor_image, positive_image, negative_image\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, train_folder, length, transform=None,):\n",
    "        self.triplet_generator = Triplet(train_folder)\n",
    "        self.transform = transform\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_image, positive_image, negative_image = self.triplet_generator.get_triplet()\n",
    "        anchor = self._load_image(anchor_image)\n",
    "        positive = self._load_image(positive_image)\n",
    "        negative = self._load_image(negative_image)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def _load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def get_triplet_names(self, index):\n",
    "        anchor_image, positive_image, negative_image = self.triplet_generator.get_triplet()\n",
    "        return anchor_image, positive_image, negative_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:02.775846Z",
     "iopub.status.busy": "2023-11-09T08:54:02.775462Z",
     "iopub.status.idle": "2023-11-09T08:54:03.514735Z",
     "shell.execute_reply": "2023-11-09T08:54:03.513581Z",
     "shell.execute_reply.started": "2023-11-09T08:54:02.775816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder: lynx_canadensis, Number of Files: 130\n",
      "Subfolder: loxodonta_cyclotis, Number of Files: 130\n",
      "Subfolder: leptailurus_serval, Number of Files: 130\n",
      "Subfolder: acinonyx_jubatus, Number of Files: 130\n",
      "Subfolder: leopardus_wiedii, Number of Files: 130\n",
      "Subfolder: felis_silvestris, Number of Files: 130\n",
      "Subfolder: leopardus_pardalis, Number of Files: 130\n",
      "Subfolder: elephas_maximus, Number of Files: 130\n",
      "Subfolder: herpailurus_yagouaroundi, Number of Files: 130\n",
      "Subfolder: felis_lybica, Number of Files: 130\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = \"/kaggle/input/animals-insects-reptiles/Animals-Insects-Reptiles/Species-Train\"\n",
    "\n",
    "# Get a list of subfolders in the specified folder\n",
    "subfolders = [subfolder for subfolder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subfolder))]\n",
    "\n",
    "total = 0\n",
    "# Iterate through the subfolders and count the number of files in each\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    num_files = len([filename for filename in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, filename))])\n",
    "    print(f\"Subfolder: {subfolder}, Number of Files: {num_files}\")\n",
    "    total += num_files\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:05.259266Z",
     "iopub.status.busy": "2023-11-09T08:54:05.258470Z",
     "iopub.status.idle": "2023-11-09T08:54:05.265784Z",
     "shell.execute_reply": "2023-11-09T08:54:05.264762Z",
     "shell.execute_reply.started": "2023-11-09T08:54:05.259231Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 16\n",
    "train_folder = \"/kaggle/input/animals-insects-reptiles/Animals-Insects-Reptiles/Species-Train\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),           # Convert to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor\n",
    "])\n",
    "train_dataset = TripletDataset(train_folder, 1300, transform=transform, )\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:14.123912Z",
     "iopub.status.busy": "2023-11-09T08:54:14.123157Z",
     "iopub.status.idle": "2023-11-09T08:54:14.568820Z",
     "shell.execute_reply": "2023-11-09T08:54:14.567956Z",
     "shell.execute_reply.started": "2023-11-09T08:54:14.123878Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class TEmbeddingNet(nn.Module):\n",
    "    def __init__(self, modelt):\n",
    "        super(TEmbeddingNet, self).__init__()\n",
    "        self.modelt = modelt\n",
    "        self.feature_extractor = nn.Sequential(*list(modelt.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return features\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        # Forward pass to get the embedding\n",
    "        return self.forward(x)\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "tmodel = TEmbeddingNet(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:16.080081Z",
     "iopub.status.busy": "2023-11-09T08:54:16.079070Z",
     "iopub.status.idle": "2023-11-09T08:54:16.086472Z",
     "shell.execute_reply": "2023-11-09T08:54:16.085563Z",
     "shell.execute_reply.started": "2023-11-09T08:54:16.080046Z"
    }
   },
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.enet = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        if x2 is None and x3 is None:\n",
    "            return self.enet.get_embedding(x1)\n",
    "        return self.enet.get_embedding(x1),self.enet.get_embedding(x2),self.enet.get_embedding(x3)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.enet.get_embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS, DEVICE, PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:17.181588Z",
     "iopub.status.busy": "2023-11-09T08:54:17.180991Z",
     "iopub.status.idle": "2023-11-09T08:54:17.187732Z",
     "shell.execute_reply": "2023-11-09T08:54:17.186795Z",
     "shell.execute_reply.started": "2023-11-09T08:54:17.181557Z"
    }
   },
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = torch.norm(anchor - positive, dim=1)\n",
    "        distance_negative = torch.norm(anchor - negative, dim=1)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:18.765919Z",
     "iopub.status.busy": "2023-11-09T08:54:18.765556Z",
     "iopub.status.idle": "2023-11-09T08:54:18.810664Z",
     "shell.execute_reply": "2023-11-09T08:54:18.809887Z",
     "shell.execute_reply.started": "2023-11-09T08:54:18.765890Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TripletNet(tmodel)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(TripletNet(tmodel))\n",
    "else:\n",
    "    model = TripletNet(tmodel)\n",
    "# Move the model to the selected device (CPU or GPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:19.918090Z",
     "iopub.status.busy": "2023-11-09T08:54:19.917694Z",
     "iopub.status.idle": "2023-11-09T08:54:19.927884Z",
     "shell.execute_reply": "2023-11-09T08:54:19.927024Z",
     "shell.execute_reply.started": "2023-11-09T08:54:19.918060Z"
    }
   },
   "outputs": [],
   "source": [
    "margin = 1\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)  # Learning rate scheduler\n",
    "loss_fn = TripletLoss(margin)\n",
    "clip_value = 0.5  # You can adjust this value as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:54:22.618888Z",
     "iopub.status.busy": "2023-11-09T08:54:22.618494Z",
     "iopub.status.idle": "2023-11-09T08:59:49.730000Z",
     "shell.execute_reply": "2023-11-09T08:59:49.728951Z",
     "shell.execute_reply.started": "2023-11-09T08:54:22.618857Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NO OF EPOCHS :  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187234163284302\n",
      "0.7841281890869141\n",
      "0.6014328002929688\n",
      "0.7398120164871216\n",
      "0.4712851047515869\n",
      "1.0727354288101196\n",
      "0.230413556098938\n",
      "0.7687028646469116\n",
      "0.6146390438079834\n",
      "Epoch 1/10, Train Loss: 0.6748, TIME: 32.89301943778992\n",
      "0.09229373931884766\n",
      "0.15297508239746094\n",
      "0.3165104389190674\n",
      "0.0\n",
      "0.5208632946014404\n",
      "0.1751692295074463\n",
      "0.11039936542510986\n",
      "0.3581583499908447\n",
      "0.5623773336410522\n",
      "Epoch 2/10, Train Loss: 0.5130, TIME: 32.65100121498108\n",
      "0.006631970405578613\n",
      "0.366219699382782\n",
      "0.31960296630859375\n",
      "0.9815921783447266\n",
      "0.530344545841217\n",
      "0.0\n",
      "0.8256067633628845\n",
      "0.9792367219924927\n",
      "0.2585333585739136\n",
      "Epoch 3/10, Train Loss: 0.4937, TIME: 32.6024694442749\n",
      "0.0\n",
      "0.3833552598953247\n",
      "0.6225736737251282\n",
      "0.14969110488891602\n",
      "0.0\n",
      "1.5482456684112549\n",
      "1.0322924852371216\n",
      "0.08744049072265625\n",
      "0.8429031372070312\n",
      "Epoch 4/10, Train Loss: 0.3654, TIME: 32.43656373023987\n",
      "0.6827393770217896\n",
      "0.0\n",
      "0.22038275003433228\n",
      "0.14798444509506226\n",
      "0.4803178310394287\n",
      "0.0\n",
      "0.23395603895187378\n",
      "1.2314666509628296\n",
      "0.5276435613632202\n",
      "Epoch 5/10, Train Loss: 0.2497, TIME: 32.70717692375183\n",
      "0.4774073362350464\n",
      "0.19316357374191284\n",
      "0.16215264797210693\n",
      "0.2925833463668823\n",
      "0.20784276723861694\n",
      "0.5030478835105896\n",
      "0.0139845609664917\n",
      "0.11611664295196533\n",
      "0.0\n",
      "Epoch 6/10, Train Loss: 0.1806, TIME: 32.50706338882446\n",
      "0.0\n",
      "0.00650179386138916\n",
      "0.08084321022033691\n",
      "0.2705427408218384\n",
      "0.0\n",
      "0.21895426511764526\n",
      "0.13983899354934692\n",
      "0.11268824338912964\n",
      "0.19347500801086426\n",
      "Epoch 7/10, Train Loss: 0.1221, TIME: 32.57459831237793\n",
      "0.0\n",
      "0.33066827058792114\n",
      "0.0\n",
      "0.6841424703598022\n",
      "0.0\n",
      "0.0\n",
      "0.09971415996551514\n",
      "0.012534022331237793\n",
      "0.16171890497207642\n",
      "Epoch 8/10, Train Loss: 0.1000, TIME: 32.37984275817871\n",
      "0.042099177837371826\n",
      "0.015122771263122559\n",
      "0.03274738788604736\n",
      "0.0\n",
      "0.35398346185684204\n",
      "0.0708543062210083\n",
      "0.13610827922821045\n",
      "0.153553307056427\n",
      "0.3088202476501465\n",
      "Epoch 9/10, Train Loss: 0.1018, TIME: 32.29743695259094\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.3194618225097656\n",
      "0.22248291969299316\n",
      "0.16690212488174438\n",
      "0.15357404947280884\n",
      "0.6066600680351257\n",
      "Epoch 10/10, Train Loss: 0.1011, TIME: 32.40801668167114\n"
     ]
    }
   ],
   "source": [
    "def fit(model, num_epochs, train_loader, bs):\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            anchor, positive, negative = batch\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_embedding, positive_embedding, negative_embedding = model(anchor, positive, negative)\n",
    "            anchor_embedding.requires_grad_(True)\n",
    "            positive_embedding.requires_grad_(True)\n",
    "            negative_embedding.requires_grad_(True)\n",
    "            loss = loss_fn(anchor_embedding, positive_embedding, negative_embedding)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            if idx%10==0:\n",
    "                print(loss.item())\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss / len(train_loader):.4f}, TIME: {time.time()-start}\")\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "fit(model, n_epochs:=int(input(\"NO OF EPOCHS : \")), train_loader, bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T08:59:54.424865Z",
     "iopub.status.busy": "2023-11-09T08:59:54.424016Z",
     "iopub.status.idle": "2023-11-09T08:59:54.433934Z",
     "shell.execute_reply": "2023-11-09T08:59:54.432938Z",
     "shell.execute_reply.started": "2023-11-09T08:59:54.424832Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        valid_extensions = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "        for class_name in os.listdir(self.folder_path):\n",
    "            class_folder = os.path.join(self.folder_path, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for filename in os.listdir(class_folder):\n",
    "                    if filename.lower().endswith(valid_extensions):\n",
    "                        self.image_paths.append(os.path.join(class_folder, filename))\n",
    "                        self.labels.append(class_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:06.970485Z",
     "iopub.status.busy": "2023-11-09T09:01:06.970106Z",
     "iopub.status.idle": "2023-11-09T09:01:07.074032Z",
     "shell.execute_reply": "2023-11-09T09:01:07.073196Z",
     "shell.execute_reply.started": "2023-11-09T09:01:06.970455Z"
    }
   },
   "outputs": [],
   "source": [
    "train_folder = \"/kaggle/input/animals-insects-reptiles/Animals-Insects-Reptiles/Species-Train\"\n",
    "train_dataloader = DataLoader(CustomDataset(train_folder,transform=transform))\n",
    "test_folder = \"/kaggle/input/animals-insects-reptiles/Animals-Insects-Reptiles/Species-Test\"\n",
    "test_dataloader = DataLoader(CustomDataset(test_folder,transform=transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T05:37:46.431660Z",
     "iopub.status.busy": "2023-11-09T05:37:46.431373Z",
     "iopub.status.idle": "2023-11-09T05:37:46.436174Z",
     "shell.execute_reply": "2023-11-09T05:37:46.435176Z",
     "shell.execute_reply.started": "2023-11-09T05:37:46.431634Z"
    }
   },
   "source": [
    "# EXTRACT EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:09.910089Z",
     "iopub.status.busy": "2023-11-09T09:01:09.909402Z",
     "iopub.status.idle": "2023-11-09T09:01:25.733305Z",
     "shell.execute_reply": "2023-11-09T09:01:25.732337Z",
     "shell.execute_reply.started": "2023-11-09T09:01:09.910058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300/1300 [00:15<00:00, 82.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.817509412765503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_embs = None\n",
    "train_labels = []\n",
    "for i in tqdm(train_dataloader):\n",
    "    I, L = i\n",
    "    train_labels.append(L)\n",
    "    emb = tmodel(I.to(device)) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if train_embs is None:\n",
    "        train_embs = emb\n",
    "    else:\n",
    "        train_embs = torch.cat((train_embs, emb), dim=0)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:28.611266Z",
     "iopub.status.busy": "2023-11-09T09:01:28.610661Z",
     "iopub.status.idle": "2023-11-09T09:01:31.601475Z",
     "shell.execute_reply": "2023-11-09T09:01:31.600445Z",
     "shell.execute_reply.started": "2023-11-09T09:01:28.611234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 67.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9841606616973877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_embs = None\n",
    "test_labels = []\n",
    "for i in tqdm(test_dataloader):\n",
    "    I, L = i\n",
    "    try:\n",
    "        emb = tmodel(I.to(device)) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "        emb = emb.detach()\n",
    "        if test_embs is None:\n",
    "            test_embs = emb\n",
    "        else:\n",
    "            test_embs = torch.cat((test_embs, emb), dim=0)\n",
    "        test_labels.append(L)\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:35.614426Z",
     "iopub.status.busy": "2023-11-09T09:01:35.614021Z",
     "iopub.status.idle": "2023-11-09T09:01:36.770956Z",
     "shell.execute_reply": "2023-11-09T09:01:36.770035Z",
     "shell.execute_reply.started": "2023-11-09T09:01:35.614397Z"
    }
   },
   "outputs": [],
   "source": [
    "embs_cpu_np = train_embs.cpu().numpy()\n",
    "embs_cpu_np = embs_cpu_np.reshape(embs_cpu_np.shape[0], -1)\n",
    "index = faiss.IndexHNSWFlat(embs_cpu_np.shape[1], 32)  # M = 32 for the HNSW index\n",
    "index.add(embs_cpu_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:39.201224Z",
     "iopub.status.busy": "2023-11-09T09:01:39.200319Z",
     "iopub.status.idle": "2023-11-09T09:01:39.208033Z",
     "shell.execute_reply": "2023-11-09T09:01:39.206994Z",
     "shell.execute_reply.started": "2023-11-09T09:01:39.201189Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_with_faiss(embs, index):\n",
    "    TOTAL = len(embs)\n",
    "    CORRECT = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize the tqdm progress bar\n",
    "    with tqdm(total=TOTAL) as pbar:\n",
    "        for idx, emb in enumerate(embs):\n",
    "            label = index.search(emb.reshape(1, -1), 1)[1][0][0]\n",
    "            if train_labels[label][0] == test_labels[idx][0]:\n",
    "                CORRECT += 1\n",
    "            pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    accuracy = (CORRECT / TOTAL) * 100\n",
    "    elapsed_time = time.time() - start\n",
    "    return f'Accuracy: {CORRECT}/{TOTAL} = {accuracy:.2f}%, Time: {elapsed_time:.2f} seconds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:41.007520Z",
     "iopub.status.busy": "2023-11-09T09:01:41.006588Z",
     "iopub.status.idle": "2023-11-09T09:01:41.012317Z",
     "shell.execute_reply": "2023-11-09T09:01:41.011455Z",
     "shell.execute_reply.started": "2023-11-09T09:01:41.007483Z"
    }
   },
   "outputs": [],
   "source": [
    "embs2_cpu_np = test_embs.cpu().numpy()\n",
    "embs2_cpu_np = embs2_cpu_np.reshape(embs2_cpu_np.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy : 56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T09:01:44.274680Z",
     "iopub.status.busy": "2023-11-09T09:01:44.274295Z",
     "iopub.status.idle": "2023-11-09T09:01:44.508991Z",
     "shell.execute_reply": "2023-11-09T09:01:44.508063Z",
     "shell.execute_reply.started": "2023-11-09T09:01:44.274651Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 883.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexHNSWFlat : Accuracy: 113/200 = 56.50%, Time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'IndexHNSWFlat : {evaluate_with_faiss(embs2_cpu_np,index)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
